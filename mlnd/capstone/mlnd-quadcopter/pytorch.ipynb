{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카트폴 게임 마스터하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym을 이용하여 게임환경 구축하기\n",
    "\n",
    "\n",
    "강화학습 예제들을 보면 항상 게임과 연관되어 있습니다. 원래 우리가 궁극적으로 원하는 목표는 어디서든 적응할 수 있는 인공지능이지만, 너무 복잡한 문제이기도 하고 가상 환경을 설계하기도 어렵기 때문에 일단 게임이라는 환경을 사용해 하는 것입니다.\n",
    "\n",
    "대부분의 게임은 점수 혹은 목표가 있습니다. 점수가 오르거나 목표에 도달하면 일종의 리워드를 받고 원치 않은 행동을 할때는 마이너스 리워드를 주는 경우도 있습니다. 아까 비유를 들었던 달리기를 배울때의 경우를 예로 들면 총 나아간 길이 혹은 목표 도착지 도착 여부로 리워드를 주고 넘어질때 패널티를 줄 수 있을 것입니다. \n",
    "\n",
    "게임중에서도 가장 간단한 카트폴이라는 환경을 구축하여 강화학습을 배울 토대를 마련해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "EPISODES = 50    # 에피소드 반복 횟수\n",
    "EPS_START = 0.9  # e-greedy threshold 시작 값\n",
    "EPS_END = 0.05   # e-greedy threshold 최종 값\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.8      # \n",
    "LR = 0.001       # NN optimizer learning rate\n",
    "BATCH_SIZE = 64  # Q-learning batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
    "        self.steps_done = 0\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if random.random() > eps_threshold:\n",
    "            return self.model(state).data.max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.LongTensor([[random.randrange(2)]])\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((state,\n",
    "                            action,\n",
    "                            torch.FloatTensor([reward]),\n",
    "                            torch.FloatTensor([next_state])))\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Experience Replay\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (GAMMA * max_next_q)\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 준비하기\n",
    "\n",
    "드디어 만들어둔 DQNAgent를 인스턴스화 합니다.\n",
    "그리고 `gym`을 이용하여 `CartPole-v0`환경도 준비합니다.\n",
    "자, 이제 `agent` 객체를 이용하여 `CartPole-v0` 환경과 상호작용을 통해 게임을 배우도록 하겠습니다.\n",
    "학습 진행을 기록하기 위해 `score_history` 리스트를 이용하여 점수를 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "env = gym.make('CartPole-v0')\n",
    "score_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드:1 점수: 11\n",
      "에피소드:2 점수: 32\n",
      "에피소드:3 점수: 10\n",
      "에피소드:4 점수: 36\n",
      "에피소드:5 점수: 13\n",
      "에피소드:6 점수: 17\n",
      "에피소드:7 점수: 9\n",
      "에피소드:8 점수: 13\n",
      "에피소드:9 점수: 11\n",
      "에피소드:10 점수: 28\n",
      "에피소드:11 점수: 11\n",
      "에피소드:12 점수: 12\n",
      "에피소드:13 점수: 9\n",
      "에피소드:14 점수: 20\n",
      "에피소드:15 점수: 11\n",
      "에피소드:16 점수: 11\n",
      "에피소드:17 점수: 10\n",
      "에피소드:18 점수: 15\n",
      "에피소드:19 점수: 13\n",
      "에피소드:20 점수: 11\n",
      "에피소드:21 점수: 13\n",
      "에피소드:22 점수: 22\n",
      "에피소드:23 점수: 26\n",
      "에피소드:24 점수: 59\n",
      "에피소드:25 점수: 30\n",
      "에피소드:26 점수: 22\n",
      "에피소드:27 점수: 26\n",
      "에피소드:28 점수: 25\n",
      "에피소드:29 점수: 57\n",
      "에피소드:30 점수: 83\n",
      "에피소드:31 점수: 62\n",
      "에피소드:32 점수: 45\n",
      "에피소드:33 점수: 62\n",
      "에피소드:34 점수: 80\n",
      "에피소드:35 점수: 88\n",
      "에피소드:36 점수: 57\n",
      "에피소드:37 점수: 52\n",
      "에피소드:38 점수: 45\n",
      "에피소드:39 점수: 49\n",
      "에피소드:40 점수: 63\n",
      "에피소드:41 점수: 61\n",
      "에피소드:42 점수: 75\n",
      "에피소드:43 점수: 52\n",
      "에피소드:44 점수: 81\n",
      "에피소드:45 점수: 98\n",
      "에피소드:46 점수: 129\n",
      "에피소드:47 점수: 153\n",
      "에피소드:48 점수: 169\n",
      "에피소드:49 점수: 120\n",
      "에피소드:50 점수: 144\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, EPISODES+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        state = torch.FloatTensor([state])\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"에피소드:{0} 점수: {1}\".format(e, steps))\n",
    "            score_history.append(steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
